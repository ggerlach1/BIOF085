---
jupyter:
  jupytext:
    formats: ipynb,Rmd
    text_representation:
      extension: .Rmd
      format_name: rmarkdown
      format_version: '1.2'
      jupytext_version: 1.5.2
  kernelspec:
    display_name: Python 3
    language: python
    name: python3
---

<!-- #region colab_type="text" id="844244jGDCuc" -->
# Machine Learning using Python

## Scikit-learn

Scikit-learn (`sklearn`) is the main Python package for machine learning. It is a widely-used and well-regarded package. However, there are a couple of challenges to using it given the usual `pandas`-based data munging pipeline. 

1. `sklearn` requires that all inputs be numeric, and in fact, `numpy` arrays.
1. `sklearn` requires that all categorical variables by replaced by 0/1 dummy variables
1. `sklearn` requires us to separate the predictors from the outcome. We need to have one `X` matrix for the predictors and one `y` vector for the outcome.

The big issue, of course, is the first point. Given we used `pandas` precisely because we wanted to be able to keep heterogenous data. We have to be able to convert non-numeric data to numeric. `pandas` does help us out with this problem. 

1. First of all, we know that all `pandas` Series and DataFrame objects can be converted to `numpy` arrays using the `values` or `to_numpy` functions. 
1. Second, we can easily extract a single variable from the data set using either the usual extracton methods or the 
`pop` function. 
1. Third, `pandas` gives us a way to convert all categorical values to numeric dummy variables using the `get_dummies` function. This is actually a more desirable solution than what you will see in cyberspace, which is to use the 
`OneHotEncoder` function from `sklearn`. 
    + This is generally fine since many machine learning models look for interactions internally and don't need them to be overtly specified. The main exceptions to this are linear and logistic regression. For those, we can use the formula methods described in the Statistical Modeling module to generate the appropriately transformed design matrix.
    + If the outcome variable is not numeric, we can `LabelEncoder` function from the `sklearn.preprocessing` submodule to convert it. 

I just threw a bunch of jargon at you. Let's see what this means.

### Transforming the outcome/target
<!-- #endregion -->

```{python 05-python-learning-1, colab={'base_uri': 'https://localhost:8080/', 'height': 206}, colab_type="code", executionInfo={'elapsed': 744, 'status': 'ok', 'timestamp': 1598819336674, 'user': {'displayName': 'Gaby Gerlach', 'photoUrl': '', 'userId': '11117434220237101401'}, 'user_tz': 240}, id="NznlV6ZdDCud", outputId="1035be86-56f8-4bf1-c68e-daab02835b24"}
import numpy as np
import pandas as pd
import sklearn
import statsmodels.api as sm
import matplotlib.pyplot as plt
import seaborn as sns

iris = sm.datasets.get_rdataset('iris').data  # pulls avalible data from within seaborn
iris.head()
```


<!-- #region colab_type="text" id="8Nb4sA76DCuo" -->
Let's hit the first issue first. We need to separate out the outcome (the variable we want to predict) from the predictors (in this case the sepal and petal measurements). 
<!-- #endregion -->

```{python 05-python-learning-2, colab={}, colab_type="code", executionInfo={'elapsed': 321, 'status': 'ok', 'timestamp': 1598819336866, 'user': {'displayName': 'Gaby Gerlach', 'photoUrl': '', 'userId': '11117434220237101401'}, 'user_tz': 240}, id="H4rfoGV3DCuq"}
y = iris['Species']
X = iris.drop('Species', axis = 1) # drops column, makes a copy
```

<!-- #region colab_type="text" id="IuLEKi_qDCuv" -->
Another way to do this is 
<!-- #endregion -->

```{python 05-python-learning-3, colab={}, colab_type="code", executionInfo={'elapsed': 877, 'status': 'ok', 'timestamp': 1598819338168, 'user': {'displayName': 'Gaby Gerlach', 'photoUrl': '', 'userId': '11117434220237101401'}, 'user_tz': 240}, id="P6ItYxawDCuw"}
y = iris.pop('Species') # Another way to do the same thing 
```

<!-- #region colab_type="text" id="DAnXTTZ4DCu0" -->
If you look at this, `iris` now only has 4 columns. So we could just use `iris` after the `pop` application, as the predictor set
<!-- #endregion -->

<!-- #region colab_type="text" id="tCeGGiw_DCu1" -->
We still have to update `y` to become numeric. This is where the `sklearn` functions start to be handy
<!-- #endregion -->

```{python 05-python-learning-4, colab={'base_uri': 'https://localhost:8080/', 'height': 139}, colab_type="code", executionInfo={'elapsed': 592, 'status': 'ok', 'timestamp': 1598819338933, 'user': {'displayName': 'Gaby Gerlach', 'photoUrl': '', 'userId': '11117434220237101401'}, 'user_tz': 240}, id="Of6UEdpyDCu1", outputId="24d8e3d0-4c29-4d80-b294-9197ef76b94e"}
from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
y = le.fit_transform(y) #makes non-numeric y numeric
y
```

<!-- #region colab_type="text" id="k-MOCotqDCu7" -->
Let's talk about this code, since it's very typical of the way the `sklearn`
code works. First, we import a method (`LabelEncoder`) from the appropriate
`sklearn` module. The second line, `le = LabelEncoder()` works to "turn on" the
method. This is like taking a power tool off the shelf and plugging it in to a
socket. It's now ready to work. The third line does the actual work.  The
`fit_transform` function transforms the data you input into it based on the
method it is then attached to.

> Let's make a quick analogy. You can plug in both a power washer and a
jackhammer to get them ready to go. You can then apply each of them to your
driveway. They "transform" the driveway in different ways depending on which
tool is used. The washer would "transform" the driveway by cleaning it, while
the jackhammer would transform the driveway by breaking it.
<!-- #endregion -->

<!-- #region colab_type="text" id="UaYV483FDCu8" -->
There's an interesting invisible quirk to the code, though. The object `le` also got transformed during this 
process. There were pieces added to it during the `fit_transform` process. 
<!-- #endregion -->

```{python 05-python-learning-5, colab={}, colab_type="code", executionInfo={'elapsed': 770, 'status': 'ok', 'timestamp': 1598819340614, 'user': {'displayName': 'Gaby Gerlach', 'photoUrl': '', 'userId': '11117434220237101401'}, 'user_tz': 240}, id="ulC5JSVlDCu9"}
le = LabelEncoder()
d1 = dir(le)
```

```{python 05-python-learning-6, colab={'base_uri': 'https://localhost:8080/', 'height': 34}, colab_type="code", executionInfo={'elapsed': 1085, 'status': 'ok', 'timestamp': 1598819341401, 'user': {'displayName': 'Gaby Gerlach', 'photoUrl': '', 'userId': '11117434220237101401'}, 'user_tz': 240}, id="r_KAnlzjDCvB", outputId="9568c9fe-13f5-433b-ac5f-8a22ccc0edc9"}
y = le.fit_transform( pd.read_csv('data/iris.csv')['species'])
d2 = dir(le)
set(d2).difference(set(d1)) # set of things in d2 but not in d1
```

<!-- #region colab_type="text" id="pQANH3ihDCvF" -->
So we see that there is a new component added, called `classes_`. 
<!-- #endregion -->

```{python 05-python-learning-7, colab={'base_uri': 'https://localhost:8080/', 'height': 34}, colab_type="code", executionInfo={'elapsed': 596, 'status': 'ok', 'timestamp': 1598819342371, 'user': {'displayName': 'Gaby Gerlach', 'photoUrl': '', 'userId': '11117434220237101401'}, 'user_tz': 240}, id="60-aYPSHDCvF", outputId="b0bc7cf9-7f67-470a-9097-ceb1f7b37dbf"}
le.classes_
```

<!-- #region colab_type="text" id="sLk9A0LhDCvK" -->
So the original labels aren't destroyed; they are being stored. This can be useful.
<!-- #endregion -->

```{python 05-python-learning-8, colab={'base_uri': 'https://localhost:8080/', 'height': 52}, colab_type="code", executionInfo={'elapsed': 345, 'status': 'ok', 'timestamp': 1598819343130, 'user': {'displayName': 'Gaby Gerlach', 'photoUrl': '', 'userId': '11117434220237101401'}, 'user_tz': 240}, id="17ZPoBIpDCvL", outputId="b0e29abe-7e85-49cf-bc4b-35b93c684c9a"}
le.inverse_transform([0,1,1,2,0]) # gets the words associated with the numerical encoding 
```

<!-- #region colab_type="text" id="dkDmLVKLDCvO" -->
So we can transform back from the numeric to the labels. Keep this in hand, since it will prove useful after
we have done some predictions using a ML model, which will give numeric predictions. 
<!-- #endregion -->

<!-- #region colab_type="text" id="SyGyT9raDCvP" -->
### Transforming the predictors

Let's look at a second example. The `diamonds` dataset has several categorical variables that would need to be transformed. 
<!-- #endregion -->

```{python 05-python-learning-9, colab={}, colab_type="code", executionInfo={'elapsed': 747, 'status': 'ok', 'timestamp': 1598819344980, 'user': {'displayName': 'Gaby Gerlach', 'photoUrl': '', 'userId': '11117434220237101401'}, 'user_tz': 240}, id="oCYJWIIeDCvQ"}
diamonds = pd.read_csv('data/diamonds.csv.gz')

y = diamonds.pop('price').values # the .values ensures we have a numpy array not a pandas series
X = pd.get_dummies(diamonds) # converts all other features to numerical values

# Alternatively
# import patsy
# f = '~ np.log(carat) +  + clarity + depth + cut * color'
# X = patsy.dmatrix(f, data=diamonds)
```

```{python}
type(y)
```

```{python 05-python-learning-10, colab={'base_uri': 'https://localhost:8080/', 'height': 34}, colab_type="code", executionInfo={'elapsed': 485, 'status': 'ok', 'timestamp': 1598819345165, 'user': {'displayName': 'Gaby Gerlach', 'photoUrl': '', 'userId': '11117434220237101401'}, 'user_tz': 240}, id="PtI-CYo8DCvU", outputId="86a63e2c-b9ae-4145-c513-25594572f3ee"}
type(X)
```

```{python 05-python-learning-11, colab={'base_uri': 'https://localhost:8080/', 'height': 590}, colab_type="code", executionInfo={'elapsed': 619, 'status': 'ok', 'timestamp': 1598819345699, 'user': {'displayName': 'Gaby Gerlach', 'photoUrl': '', 'userId': '11117434220237101401'}, 'user_tz': 240}, id="IdalfF5_DCvX", outputId="0cc1a481-c328-4761-d54c-8318bed7357b"}
X.info()
```

<!-- #region colab_type="text" id="WC7hU7VwDCvc" -->
So everything is now numeric!!. Let's take a peek inside.
<!-- #endregion -->

```{python 05-python-learning-12, colab={'base_uri': 'https://localhost:8080/', 'height': 121}, colab_type="code", executionInfo={'elapsed': 620, 'status': 'ok', 'timestamp': 1598819346635, 'user': {'displayName': 'Gaby Gerlach', 'photoUrl': '', 'userId': '11117434220237101401'}, 'user_tz': 240}, id="0KX0y1UWDCvd", outputId="82f6ed29-2dc9-4365-a5f1-b0f94de609d0"}
X.columns
```

<!-- #region colab_type="text" id="n8zy3YBoDCvg" -->
So, it looks like the continuous variables remain intact, but the categorical variables got exploded out. Each
variable name has a level with it, which represents the particular level it is representing. Each of these 
variables, called dummy variables, are numerical 0/1 variables. For example, `color_F` is 1 for those diamonds which have color F, and 0 otherwise. 
<!-- #endregion -->

```{python}
help(pd.crosstab)
```

```{python 05-python-learning-13, colab={'base_uri': 'https://localhost:8080/', 'height': 143}, colab_type="code", executionInfo={'elapsed': 730, 'status': 'ok', 'timestamp': 1598819348031, 'user': {'displayName': 'Gaby Gerlach', 'photoUrl': '', 'userId': '11117434220237101401'}, 'user_tz': 240}, id="GIu2yMgdDCvg", outputId="b482c563-af7a-4f3e-b06e-dac5cc6b841f"}
pd.crosstab(X['color_F'], diamonds['color']) 
#Compute a simple cross tabulation of two (or more) factors. By default
#computes a frequency table of the factors unless an array of values and an aggregation function are passed.

# here: in the new representation "X" the value of color_F is zero everywhere execpt the locations where 
# in the old representation "dimonds" the color column is labeled "F"
```

<!-- #region colab_type="text" id="z7O3X1cZDCvl" -->
## Supervised Learning

We will first look at supervised learning methods. 

| ML method               | Code to call it                                              |
| ----------------------- | ------------------------------------------------------------ |
| Decision Tree           | `sklearn.tree.DecisionTreeClassifier`, `sklearn.tree.DecisionTreeRegressor` |
| Random Forest           | `sklearn.ensemble.RandomForestClassifier`, `sklearn.ensemble.RandomForestRegressor` |
| Linear Regression       | `sklearn.linear_model.LinearRegression`                      |
| Logistic Regression     | `sklearn.linear_model.LogisticRegression`                    |
| Support Vector Machines | `sklearn.svm.LinearSVC`, `sklearn.svm.LinearSVR`             |
|                         |                                                              |
<!-- #endregion -->

<!-- #region colab_type="text" id="KUsteQj1DCvl" -->
The general method that the code will follow is :

```
from sklearn.... import Machine
machine = Machine(*parameters*)
machine.fit(X, y)
```

### A quick example
<!-- #endregion -->

```{python 05-python-learning-14, colab={}, colab_type="code", executionInfo={'elapsed': 1045, 'status': 'ok', 'timestamp': 1598819349714, 'user': {'displayName': 'Gaby Gerlach', 'photoUrl': '', 'userId': '11117434220237101401'}, 'user_tz': 240}, id="X_S5IRQwDCvl"}
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor

# initiating both linear regression and decision tree 
lm = LinearRegression()
dt = DecisionTreeRegressor()
```

<!-- #region colab_type="text" id="9vDp6YjUDCvp" -->
Lets manufacture some data
<!-- #endregion -->

```{python}
help(lm.predict)
```

```{python 05-python-learning-15, colab={'base_uri': 'https://localhost:8080/', 'height': 369}, colab_type="code", executionInfo={'elapsed': 1468, 'status': 'ok', 'timestamp': 1598819351110, 'user': {'displayName': 'Gaby Gerlach', 'photoUrl': '', 'userId': '11117434220237101401'}, 'user_tz': 240}, id="bKp4NhloDCvp", outputId="fec8a316-f5d2-471e-ee12-38b2a8e6edce"}
# create a function 
x = np.linspace(0, 10, 200)
y = 2 + 3*x - 5*(x**2) # y is a non-linear function of x 
d = pd.DataFrame({'x': x}) # create a dataframe of x 

lm.fit(d,y) # Using linear regression fit d to y (training)
dt.fit(d, y) # Using a decision Tree fit d to y (training)

p1 = lm.predict(d) #prediction using trained model, Linear regression 
p2 = dt.predict(d) #prediction using trained model, decision tree 

# add to dataframe d for plotting without ground truth 
d['lm'] = p1
d['dt'] = p2


D = pd.melt(d, id_vars = 'x')

sns.relplot(data=D, x = 'x', y = 'value', hue = 'variable')
plt.show()

#with ground truth, y and the dt overlap perfectly!! 
d['lm'] = p1
d['dt'] = p2
d['y'] = y
D = pd.melt(d, id_vars = 'x')

sns.relplot(data=D, x = 'x', y = 'value', hue = 'variable')
plt.show()
```


From this we note that linear regression is unable to fit a non-linear function, but the decision tree can

<!-- #region colab_type="text" id="swJ1Fz1oDCvt" -->
### A data analytic example
<!-- #endregion -->

```{python 05-python-learning-16, colab={'base_uri': 'https://localhost:8080/', 'height': 312}, colab_type="code", executionInfo={'elapsed': 738, 'status': 'ok', 'timestamp': 1598819352484, 'user': {'displayName': 'Gaby Gerlach', 'photoUrl': '', 'userId': '11117434220237101401'}, 'user_tz': 240}, id="RyE-YxaiDCvt", outputId="84f451c2-0105-439c-abde-7c07dfcd605e"}
diamonds = pd.read_csv('data/diamonds.csv.gz')
diamonds.info()
```

<!-- #region colab_type="text" id="Q-NQvs3EDCvw" -->
First, lets separate out the outcome (price) and the predictors
<!-- #endregion -->

```{python 05-python-learning-17, colab={}, colab_type="code", executionInfo={'elapsed': 812, 'status': 'ok', 'timestamp': 1598819380525, 'user': {'displayName': 'Gaby Gerlach', 'photoUrl': '', 'userId': '11117434220237101401'}, 'user_tz': 240}, id="R8s3n5ApDCvx"}
y = diamonds.pop('price')
```

<!-- #region colab_type="text" id="0aP1F-AYDCv0" -->
For many machine learning problems, it is useful to scale the numeric predictors so that they have mean 0 and 
variance 1. First we need to separate out the categorical and numeric variables
<!-- #endregion -->

```{python 05-python-learning-18, colab={}, colab_type="code", executionInfo={'elapsed': 740, 'status': 'ok', 'timestamp': 1598819383634, 'user': {'displayName': 'Gaby Gerlach', 'photoUrl': '', 'userId': '11117434220237101401'}, 'user_tz': 240}, id="F_G6nlh2DCv1"}
d1 = diamonds.select_dtypes(include = 'number') #numeric variables 
d2 = diamonds.select_dtypes(exclude = 'number') #categorial variables 
```

<!-- #region colab_type="text" id="7uzDtSztDCv3" -->
Now let's scale the columns of `d1`
<!-- #endregion -->

```{python 05-python-learning-19, colab={'base_uri': 'https://localhost:8080/', 'height': 243}, colab_type="code", executionInfo={'elapsed': 750, 'status': 'ok', 'timestamp': 1598819385357, 'user': {'displayName': 'Gaby Gerlach', 'photoUrl': '', 'userId': '11117434220237101401'}, 'user_tz': 240}, id="RDL3KMrEDCv6", outputId="d6c0c3c9-3607-4e1a-8f45-2bacb469bdac"}
from sklearn.preprocessing import scale

bl = scale(d1) #scalling the numerical values so they have mean 0 and varience 1 
bl
```

<!-- #region colab_type="text" id="3y8aLP0ODCv-" -->
Woops!! We get a `numpy` array, not a `DataFrame`!!
<!-- #endregion -->

```{python 05-python-learning-20, colab={}, colab_type="code", executionInfo={'elapsed': 956, 'status': 'ok', 'timestamp': 1598819386442, 'user': {'displayName': 'Gaby Gerlach', 'photoUrl': '', 'userId': '11117434220237101401'}, 'user_tz': 240}, id="JxYRwDMuDCv_"}
bl = pd.DataFrame(scale(d1))
bl.columns = list(d1.columns)
d1 = bl
```

<!-- #region colab_type="text" id="9HY98EIIDCwC" -->
Now, let's recode the categorical variables into dummy variables.
<!-- #endregion -->

```{python 05-python-learning-21, colab={}, colab_type="code", executionInfo={'elapsed': 848, 'status': 'ok', 'timestamp': 1598819387334, 'user': {'displayName': 'Gaby Gerlach', 'photoUrl': '', 'userId': '11117434220237101401'}, 'user_tz': 240}, id="dVnR5i1CDCwC"}
d2 = pd.get_dummies(d2) # change the categorical variables to numerical representation 
```

<!-- #region colab_type="text" id="SnTyXLU2DCwG" -->
and put them back together
<!-- #endregion -->

```{python 05-python-learning-22, colab={}, colab_type="code", executionInfo={'elapsed': 826, 'status': 'ok', 'timestamp': 1598819388189, 'user': {'displayName': 'Gaby Gerlach', 'photoUrl': '', 'userId': '11117434220237101401'}, 'user_tz': 240}, id="aCZK21jeDCwG"}
X = pd.concat([d1,d2], axis = 1) #combine back into one dataframe
```

<!-- #region colab_type="text" id="d390i5TvDCwI" -->
Next we need to split the data into a training set and a test set. Usually we do this as an 80/20 split. 
The purpose of the test set is to see how well the model works on an "external" data set. We don't touch the 
test set until we're done with all our model building in the training set. We usually do the split using 
random numbers. We'll put 40,000 observations in the training set.

<!-- #endregion -->

```{python 05-python-learning-23, colab={}, colab_type="code", executionInfo={'elapsed': 998, 'status': 'ok', 'timestamp': 1598819389285, 'user': {'displayName': 'Gaby Gerlach', 'photoUrl': '', 'userId': '11117434220237101401'}, 'user_tz': 240}, id="Vb4IoHShDCwJ"}
ind = list(X.index)
np.random.shuffle(ind) #randomize the indexes we are selecting

X_train, y_train = X.loc[ind[:40000],:], y[ind[:40000]]
X_test, y_test = X.loc[ind[40000:],:], y[ind[40000:]]
```


<!-- #region colab_type="text" id="Ny93PWTyDCwM" -->
There is another way to do this
<!-- #endregion -->

```{python 05-python-learning-24, colab={}, colab_type="code", executionInfo={'elapsed': 817, 'status': 'ok', 'timestamp': 1598819389937, 'user': {'displayName': 'Gaby Gerlach', 'photoUrl': '', 'userId': '11117434220237101401'}, 'user_tz': 240}, id="52JkPSa4DCwM"}
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y , test_size = 0.2, random_state= 40) 
#this one line does what the above several do!! 
```

<!-- #region colab_type="text" id="5WvPzU8mDCwO" -->
Now we will fit our models to the training data. Let's use a decision tree model, a random forest model, and a linear regression.
<!-- #endregion -->

```{python 05-python-learning-25, colab={}, colab_type="code", executionInfo={'elapsed': 885, 'status': 'ok', 'timestamp': 1598819390944, 'user': {'displayName': 'Gaby Gerlach', 'photoUrl': '', 'userId': '11117434220237101401'}, 'user_tz': 240}, id="mV0n4XXEDCwQ"}
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor

#initialize models
lm = LinearRegression()
dt = DecisionTreeRegressor()
rf = RandomForestRegressor()
```


<!-- #region colab_type="text" id="3I2q87e2DCwS" -->
Now we will use our training data to fit the models
<!-- #endregion -->

```{python 05-python-learning-26, results='hide', colab={}, colab_type="code", executionInfo={'elapsed': 28455, 'status': 'ok', 'timestamp': 1598819419362, 'user': {'displayName': 'Gaby Gerlach', 'photoUrl': '', 'userId': '11117434220237101401'}, 'user_tz': 240}, id="_aJz_HKmDCwU"}
# use the fit command to train the models on the training data
lm.fit(X_train, y_train)
dt.fit(X_train, y_train)
rf.fit(X_train, y_train)
```


<!-- #region colab_type="text" id="qmkfL9qbDCwX" -->
We now need to see how well the model fit the data. We'll use the R2 statistic to be our metric of choice to evaluate the model fit.
<!-- #endregion -->

```{python 05-python-learning-27, colab={'base_uri': 'https://localhost:8080/', 'height': 143}, colab_type="code", executionInfo={'elapsed': 28883, 'status': 'ok', 'timestamp': 1598819420584, 'user': {'displayName': 'Gaby Gerlach', 'photoUrl': '', 'userId': '11117434220237101401'}, 'user_tz': 240}, id="ceZ4lxaODCwX", outputId="0168ca48-5dba-4620-c058-b995c726a215"}
from sklearn.metrics import  r2_score
# bellow we predict on the training data and get the R2 score all in one go! 

pd.DataFrame({
  'Model': ['Linear regression','Decision tree','Random forest'],
  'R2': [r2_score(y_train, lm.predict(X_train)),
    r2_score(y_train, dt.predict(X_train)),
    r2_score(y_train, rf.predict(X_train))]
})


```

<!-- #region colab_type="text" id="FZJClCXuDCwZ" -->
This is pretty amazing. However, we know that if we try and predict using the same data we used to train 
the model, we get better than expected results. One way to get a better idea about the true performance of the 
model when we will try it on external data is to do cross-validation. 
<!-- #endregion -->

<!-- #region colab_type="text" id="ug2q3oRrDCwZ" -->
### Visualizing a decision tree  (End of Lecture) 

Bellow are useful examples some of which will be covered in the videos and some are provided for further learning. The code is less commented, but we are confident you will be able to use your new skills to understand it! 

**scikit-learn** provides a decent way of visualizing a decision tree using a program called _Graphviz_, which is a dedicated graph and network visualization program.
<!-- #endregion -->

```{python results='hide', colab={'base_uri': 'https://localhost:8080/', 'height': 540}, colab_type="code", executionInfo={'elapsed': 19433, 'status': 'ok', 'timestamp': 1598819420788, 'user': {'displayName': 'Gaby Gerlach', 'photoUrl': '', 'userId': '11117434220237101401'}, 'user_tz': 240}, id="gZbiqXJ3DCwZ", message=FALSE, outputId="679e650b-387d-4fc0-f8be-ca04fb2edbf5", warning=FALSE}
import graphviz
from sklearn import tree

dt = DecisionTreeRegressor(max_depth=3) #initalize decsion tree that is only allowed 3 splits
dt.fit(X_train, y_train) #train the model 

#visualize the decsion tree 
dot_data = tree.export_graphviz(dt, out_file=None, 
                                feature_names = X_train.columns,
                                filled=True, rounded=True)
graph = graphviz.Source(dot_data)
graph
```
```{python colab={}, colab_type="code", executionInfo={'elapsed': 19103, 'status': 'ok', 'timestamp': 1598819420789, 'user': {'displayName': 'Gaby Gerlach', 'photoUrl': '', 'userId': '11117434220237101401'}, 'user_tz': 240}, id="6EIVsNb-DCwc"}
graph.render('graphs/image', view=False, format='pdf');
graph.render('graphs/image', view = False, format = 'png');
```

<!-- #region colab_type="text" id="Gb0U0QmSDCwf" -->
### Cross-validation

In cross-validation, we split the dataset up into 5  equal parts randomly. We then train the
model using 4 parts and predict the data on the 5th part. We do for all possible groups of 4 parts. We then
consider the overall performance of prediction.

![](graphs/CV5.png)

There is nothing special about the 5 splits. If you use 5 splits, it is called 5-fold cross-validatio    n (CV), if you use 10 splits, it is 10-fold CV. If you use all but one subject as training data, and     that one subject as test data, and cycle through all the subjects, that is called leave-one-out CV (L    OOCV). All these methods are widely used, but 5- and 10-fold CV are often used as a balance between e    ffectiveness and computational efficiency.

**scikit-learn** makes this pretty easy, using the `cross_val_score` function. 

<!-- #endregion -->

```{python colab={'base_uri': 'https://localhost:8080/', 'height': 36}, colab_type="code", executionInfo={'elapsed': 18063, 'status': 'ok', 'timestamp': 1598819421294, 'user': {'displayName': 'Gaby Gerlach', 'photoUrl': '', 'userId': '11117434220237101401'}, 'user_tz': 240}, id="8mtjegkLDCwg", outputId="5974b979-a7b4-4e43-9db5-37a3c0090870"}
from sklearn.model_selection import cross_val_score


cv_score = cross_val_score(dt, X_train, y_train, cv=5, scoring='r2')
f"CV error = {np.round(np.mean(cv_score), 3)}"
```

<!-- #region colab_type="text" id="Pa1Krt-gDCwi" -->
### Improving models through cross-validation

The cross-validation error, as we've seen, gives us a better estimate of  
how well our model predicts on new data. We can use this to tune models by tweaking their parameters to get models that reasonably will perform better. 

Each model that we fit has a set of parameters that govern how it proceeds
to fit the data. These can bee seen using the `get_params` function. 
<!-- #endregion -->

```{python colab={'base_uri': 'https://localhost:8080/', 'height': 34}, colab_type="code", executionInfo={'elapsed': 816, 'status': 'ok', 'timestamp': 1598819436751, 'user': {'displayName': 'Gaby Gerlach', 'photoUrl': '', 'userId': '11117434220237101401'}, 'user_tz': 240}, id="3nNNyXnmDCwi", outputId="4011d6a9-dc2c-47ef-97b9-cbb5a96338df"}
dt.get_params()
lm.get_params()
```

<!-- #region colab_type="text" id="MM5RGrD4DCwk" -->
Linear regression is entirely determined by the functional form of  
the prediction equation,i.e., the "formula" we use. It doesn't have any parameters to tune per se. Improving a linear regression involves playing 
with the different predictors and transforming them to improve the predictions. This involve subjects called _regression diagnostics_ and 
_feature engineering_ that we will leave to Google for now. 


We can tune different parameters for the decision tree to try and see if
some combination of parameters can improve predictions. One way to do this,
since we're using a computer, is a grid search. This means that we can set out sets of values of the parameters we want to tune, and the computer will go through every combination of those values to see how the model
performs, and will provide the "best" model.

We would specify the values as a dictionary to the function `GridSearchCV`, which would optimize based on the cross-validation error.
<!-- #endregion -->

```{python colab={'base_uri': 'https://localhost:8080/', 'height': 34}, colab_type="code", executionInfo={'elapsed': 14040, 'status': 'ok', 'timestamp': 1598819450727, 'user': {'displayName': 'Gaby Gerlach', 'photoUrl': '', 'userId': '11117434220237101401'}, 'user_tz': 240}, id="dKpV_-OmDCwk", outputId="c66d2601-07b0-4d6f-fac0-6741e8aed868"}
from sklearn.model_selection import GridSearchCV
import numpy.random as rnd
rnd.RandomState(39358)

param_grid = {'max_depth': [1,3,5,7, 10], 'min_samples_leaf': [1,5,10,20],
  'max_features' : ['auto','sqrt']}

clf = GridSearchCV(dt, param_grid, scoring = 'r2', cv = 5) # Tuning dt
clf.fit(X_train, y_train)

clf.best_estimator_
print(clf.best_score_)
```

<!-- #region colab_type="text" id="e2yMiYHODCwn" -->
So how does this do on the test set?
<!-- #endregion -->

```{python colab={'base_uri': 'https://localhost:8080/', 'height': 34}, colab_type="code", executionInfo={'elapsed': 13279, 'status': 'ok', 'timestamp': 1598819450728, 'user': {'displayName': 'Gaby Gerlach', 'photoUrl': '', 'userId': '11117434220237101401'}, 'user_tz': 240}, id="tiAXtwysDCwn", outputId="111c97f7-29f0-42bd-8a33-9baabbeddf4e"}
p = clf.best_estimator_.predict(X_test)
r2_score(y_test, p)
```

<!-- #region colab_type="text" id="KEPr3Q_bDCwo" -->
So this predictor is doing slightly better on the test set than the training set. This is often an indicator that the model is overfitting on the data. This is probable here, given the extremely high R2 values for this model.


### Feature selection

We can also use cross-validation to do recursive feature selection (or
backwards elimination), based on a predictive score. This is different
from usual stepwise selection methods which are based on a succession of
hypothesis tests.
<!-- #endregion -->

```{python colab={'base_uri': 'https://localhost:8080/', 'height': 69}, colab_type="code", executionInfo={'elapsed': 14244, 'status': 'ok', 'timestamp': 1598819452677, 'user': {'displayName': 'Gaby Gerlach', 'photoUrl': '', 'userId': '11117434220237101401'}, 'user_tz': 240}, id="Nwc9dZhKDCwp", outputId="52ce482f-9e3c-4371-f282-a75f2d4273e6"}
from sklearn.feature_selection import RFECV

selector = RFECV(lm, cv = 5, scoring = 'r2')
selector = selector.fit(X_train, y_train)
selector.support_
```

<!-- #region colab_type="text" id="z7jlntVVDCwq" -->
The support gives the set of predictors (True) that are finally selected.
<!-- #endregion -->

```{python colab={'base_uri': 'https://localhost:8080/', 'height': 104}, colab_type="code", executionInfo={'elapsed': 13707, 'status': 'ok', 'timestamp': 1598819452677, 'user': {'displayName': 'Gaby Gerlach', 'photoUrl': '', 'userId': '11117434220237101401'}, 'user_tz': 240}, id="P4VhefpBDCwr", outputId="73909819-8d9b-4511-b6de-661c52568ac3"}
X_train.columns[selector.support_]
```

<!-- #region colab_type="text" id="CJrnigogDCwt" -->
This is indicating that the best predictive model for the linear regression includes carat, cut, color and clarity, and width of the stone.

### Logistic regression

We noted that logistic regression is available both through **statsmodels** and through **scikit-learn**. Let's now try to fit a
logistic regression model using **scikit-learn**. We will use the same
Titanic dataset we used earlier.
<!-- #endregion -->

```{python colab={'base_uri': 'https://localhost:8080/', 'height': 243}, colab_type="code", executionInfo={'elapsed': 13456, 'status': 'ok', 'timestamp': 1598819452957, 'user': {'displayName': 'Gaby Gerlach', 'photoUrl': '', 'userId': '11117434220237101401'}, 'user_tz': 240}, id="H6GRw3KDDCwt", outputId="71e44ed4-e5cb-4687-f80a-e9de2ae0fbcb"}
import pandas as pd
import statsmodels.api as sm
import statsmodels.formula.api as smf
from sklearn.linear_model import LogisticRegression

titanic = sm.datasets.get_rdataset('Titanic','Stat2Data').data.dropna()
titanic.info()
```

<!-- #region colab_type="text" id="jIra_Ik2DCwv" -->
We will model `Survived` on the age, sex and passenger class of passengers.
<!-- #endregion -->

```{python colab={'base_uri': 'https://localhost:8080/', 'height': 104}, colab_type="code", executionInfo={'elapsed': 12910, 'status': 'ok', 'timestamp': 1598819452958, 'user': {'displayName': 'Gaby Gerlach', 'photoUrl': '', 'userId': '11117434220237101401'}, 'user_tz': 240}, id="yMmKynfsDCww", outputId="828ad317-1659-4b22-9f72-d5ee441bfaac"}
from sklearn.model_selection import train_test_split

X = pd.get_dummies(titanic[['Age','Sex','PClass']], drop_first=True)
y = titanic.Survived

X_train, X_test, y_train, y_test = train_test_split(X, y , test_size = 0.2, random_state= 40) # 80/20 split

lrm = LogisticRegression()
lrm.fit(X_train, y_train)
```

<!-- #region colab_type="text" id="eyLMi-KSDCw8" -->
There are a few differences that are now evident between this model and
the model we fit using **statsmodels**. As a reminder, we fit this model again below.
<!-- #endregion -->

```{python colab={'base_uri': 'https://localhost:8080/', 'height': 359}, colab_type="code", executionInfo={'elapsed': 12442, 'status': 'ok', 'timestamp': 1598819452959, 'user': {'displayName': 'Gaby Gerlach', 'photoUrl': '', 'userId': '11117434220237101401'}, 'user_tz': 240}, id="JqxpzESaDCw9", outputId="5944ba2f-fc63-45ad-e8a6-c532e56adfa5"}
titanic1 = titanic.loc[X_train.index,:]
titanic2 = titanic.loc[X_test.index,:]
mod_logistic = smf.glm('Survived ~ Age + Sex + PClass', data=titanic1,
  family = sm.families.Binomial()).fit()
mod_logistic.summary()
```

<!-- #region colab_type="text" id="oCtnsuBLDCxA" -->
We can see the objects that are available to us from the two models using
`dir(lrm)` and `dir(mod_logistic)`. We find that `lrm` does not give us
any parameter estimates, p-values or summary methods. It is much leaner, and, in line with other machine learning models, emphasizes predictions. So if you want to find associations between predictors and outcome, you will have to use the **statsmodels** version.

Let's compare the predictions.
<!-- #endregion -->

```{python colab={'base_uri': 'https://localhost:8080/', 'height': 295}, colab_type="code", executionInfo={'elapsed': 12650, 'status': 'ok', 'timestamp': 1598819453645, 'user': {'displayName': 'Gaby Gerlach', 'photoUrl': '', 'userId': '11117434220237101401'}, 'user_tz': 240}, id="RxTamU2lDCxA", outputId="fd739752-83bd-4355-995e-c371282cfa52"}
plt.clf()
p1 = lrm.predict_proba(X_test)[:,1]
p2 = mod_logistic.predict(titanic2)

plt.plot(p1, p2, '.')
plt.plot([0,1],[0,1], ':')
plt.xlabel('scikit-learn')
plt.ylabel('statsmodels')
plt.title('Predictions')
plt.show()
```

<!-- #region colab_type="text" id="-iYHLOjSDCxB" -->
First note that the prediction functions work a bit differently. For `lrm` we have to explicitly ask for the probability predictions, whereas those are automatically provided for `mod_logistic`. We also find that the predictions aren't exactly the same. This is because `lrm`, by default, runs a penalized regression using the lasso criteria (L2 norm), rather than the non-penalized version that `mod_logistic` runs. We can specify no penalty for `lrm` and can see much closer agreement between the two models.
<!-- #endregion -->

```{python colab={'base_uri': 'https://localhost:8080/', 'height': 295}, colab_type="code", executionInfo={'elapsed': 12151, 'status': 'ok', 'timestamp': 1598819453646, 'user': {'displayName': 'Gaby Gerlach', 'photoUrl': '', 'userId': '11117434220237101401'}, 'user_tz': 240}, id="yBVwmOyXDCxB", outputId="76e5afb2-6be6-4207-f11f-86a2143f7749"}
lrm = LogisticRegression(penalty='none')
lrm.fit(X_train, y_train)
p1 = lrm.predict_proba(X_test)[:,1]

plt.clf()
plt.plot(p1, p2, '.')
plt.plot([0,1],[0,1], ':')
plt.xlabel('scikit-learn')
plt.ylabel('statsmodels')
plt.title('Predictions')
plt.show()
```

<!-- #region colab_type="text" id="oC8q9DJvDCxD" -->
## Unsupervised learning


Unsupervised learning is a class of machine learning methods where we are just trying to identify patterns in the data without any labels. This is in contrast to _supervised learning_, which are the modeling methods we have discussed above.

Most unsupervised learning methods fall broadly into a set of algorithms called _cluster analysis_. **scikit-learn** provides several clustering algorithms.


![](graphs/cluster_choice.png)


We will demonstrate the two more popular choices -- K-Means and Agglomerative clustering (also known as hierarchical clustering). We will use the classic Fisher's Iris data for this demonstration.
<!-- #endregion -->

```{python colab={'base_uri': 'https://localhost:8080/', 'height': 369}, colab_type="code", executionInfo={'elapsed': 12238, 'status': 'ok', 'timestamp': 1598819454163, 'user': {'displayName': 'Gaby Gerlach', 'photoUrl': '', 'userId': '11117434220237101401'}, 'user_tz': 240}, id="p5IHSNFPDCxE", outputId="de2a4347-624a-4f0f-e678-c8bdd2612bac"}
import statsmodels.api as sm
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.cluster import KMeans, AgglomerativeClustering

iris = sm.datasets.get_rdataset('iris').data
sns.relplot(data=iris, x = 'Sepal.Length',y = 'Sepal.Width', hue = 'Species');

```

<!-- #region colab_type="text" id="TkK-Bw0mDCxG" -->
The K-Means algorithm takes a pre-specified number of clusters as input, and then tries to find contiguous regions of the data to parse into clusters.
<!-- #endregion -->

```{python colab={}, colab_type="code", executionInfo={'elapsed': 11379, 'status': 'ok', 'timestamp': 1598819454164, 'user': {'displayName': 'Gaby Gerlach', 'photoUrl': '', 'userId': '11117434220237101401'}, 'user_tz': 240}, id="_amySV5WDCxG"}
km = KMeans(n_clusters = 3)
km.fit(iris[['Sepal.Length','Sepal.Width']]);
```

```{python colab={'base_uri': 'https://localhost:8080/', 'height': 139}, colab_type="code", executionInfo={'elapsed': 11103, 'status': 'ok', 'timestamp': 1598819454164, 'user': {'displayName': 'Gaby Gerlach', 'photoUrl': '', 'userId': '11117434220237101401'}, 'user_tz': 240}, id="BRe8Ywk_DCxI", outputId="3abcf829-69d8-4822-b68a-0a16dffb07b0"}
km.labels_
```

```{python colab={'base_uri': 'https://localhost:8080/', 'height': 369}, colab_type="code", executionInfo={'elapsed': 11597, 'status': 'ok', 'timestamp': 1598819454942, 'user': {'displayName': 'Gaby Gerlach', 'photoUrl': '', 'userId': '11117434220237101401'}, 'user_tz': 240}, id="3dUSj74yDCxL", outputId="cc3d4fcd-e930-414b-f719-163db277fc52"}
iris['km_labels'] = km.labels_
iris['km_labels'] = iris.km_labels.astype('category')

sns.relplot(data=iris, x = 'Sepal.Length', y = 'Sepal.Width',
           hue = 'km_labels');
```

<!-- #region colab_type="text" id="ssoKvArvDCxN" -->
Agglomerative clustering takes a different approach. It starts by coalescing individual points successively, based on a distance metric and a principle for how to coalesce groups of points (called _linkage_). The number of clusters can then be determined either visually or via different cutoffs.
<!-- #endregion -->

```{python colab={'base_uri': 'https://localhost:8080/', 'height': 69}, colab_type="code", executionInfo={'elapsed': 11169, 'status': 'ok', 'timestamp': 1598819454943, 'user': {'displayName': 'Gaby Gerlach', 'photoUrl': '', 'userId': '11117434220237101401'}, 'user_tz': 240}, id="rbQZb09GDCxN", outputId="f46cd1d5-f49f-49a9-eb61-695772a9f9f8"}
hc = AgglomerativeClustering(distance_threshold=0, n_clusters=None,
                             linkage='complete')

hc.fit(iris[['Sepal.Length','Sepal.Width']])
```

```{python colab={'base_uri': 'https://localhost:8080/', 'height': 36}, colab_type="code", executionInfo={'elapsed': 10953, 'status': 'ok', 'timestamp': 1598819454944, 'user': {'displayName': 'Gaby Gerlach', 'photoUrl': '', 'userId': '11117434220237101401'}, 'user_tz': 240}, id="70E2AjAQDCxQ", outputId="6be0af4f-dbf8-40fa-bcca-aac01b176df1"}
hc.linkage
```

```{python colab={'base_uri': 'https://localhost:8080/', 'height': 282}, colab_type="code", executionInfo={'elapsed': 10726, 'status': 'ok', 'timestamp': 1598819454945, 'user': {'displayName': 'Gaby Gerlach', 'photoUrl': '', 'userId': '11117434220237101401'}, 'user_tz': 240}, id="0ixkacRZDCxS", outputId="96e5fb61-dd22-4030-e7b2-d38d04ce691e"}
from scipy.cluster.hierarchy import dendrogram

## The following is from https://scikit-learn.org/stable/auto_examples/cluster/plot_agglomerative_dendrogram.html
def plot_dendrogram(model, **kwargs):
    # Create linkage matrix and then plot the dendrogram

    # create the counts of samples under each node
    counts = np.zeros(model.children_.shape[0])
    n_samples = len(model.labels_)
    for i, merge in enumerate(model.children_):
        current_count = 0
        for child_idx in merge:
            if child_idx < n_samples:
                current_count += 1  # leaf node
            else:
                current_count += counts[child_idx - n_samples]
        counts[i] = current_count

    linkage_matrix = np.column_stack([model.children_, model.distances_,
                                      counts]).astype(float)

    # Plot the corresponding dendrogram
    dendrogram(linkage_matrix, **kwargs)

plot_dendrogram(hc, truncate_mode='level', p=3)
plt.xlabel("Number of points in node (or index of point if no parenthesis).")
plt.show()

```

```{python colab={}, colab_type="code", executionInfo={'elapsed': 10805, 'status': 'ok', 'timestamp': 1598819455275, 'user': {'displayName': 'Gaby Gerlach', 'photoUrl': '', 'userId': '11117434220237101401'}, 'user_tz': 240}, id="WOneBr1XDCxU"}
hc = AgglomerativeClustering( n_clusters=3,
                             linkage='average')

hc.fit(iris[['Sepal.Length','Sepal.Width']]);

```

```{python colab={'base_uri': 'https://localhost:8080/', 'height': 139}, colab_type="code", executionInfo={'elapsed': 10601, 'status': 'ok', 'timestamp': 1598819455276, 'user': {'displayName': 'Gaby Gerlach', 'photoUrl': '', 'userId': '11117434220237101401'}, 'user_tz': 240}, id="4x9nBj1LDCxW", outputId="0173e176-b686-4704-d5be-741767b8fde9"}
hc.labels_
```

```{python colab={'base_uri': 'https://localhost:8080/', 'height': 369}, colab_type="code", executionInfo={'elapsed': 10391, 'status': 'ok', 'timestamp': 1598819455278, 'user': {'displayName': 'Gaby Gerlach', 'photoUrl': '', 'userId': '11117434220237101401'}, 'user_tz': 240}, id="TtTT24GbDCxY", outputId="a73e396b-dcc8-4dc1-8b76-10b5ad798470"}
iris['hc_labels'] = pd.Series(hc.labels_).astype('category')

sns.relplot(data=iris, x = 'Sepal.Length', y= 'Sepal.Width',
           hue = 'hc_labels');

```

<!-- #region colab_type="text" id="6t2RDyjnDCxZ" -->
Play around with different linkage methods to see how these clusters change.
<!-- #endregion -->
